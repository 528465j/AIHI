{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca33cf61-a525-47cb-b3fb-6bee024ffeb7",
   "metadata": {},
   "source": [
    "### **1. <u>Project Setup<u/>:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9762263f-404b-4acd-99ca-a8d3425d3c20",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\users\\jatin\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca02d96-5191-4355-9600-9909f855c77d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jatin\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\jatin\\anaconda3\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jatin\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jatin\\anaconda3\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\jatin\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jatin\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\jatin\\anaconda3\\lib\\site-packages (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from torch) (2023.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jatin\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision numpy matplotlib pandas scikit-learn pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef2059e1-81e6-4664-83b4-671d7c664cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device available: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2baf2052-b0eb-454d-8810-9d4fa7e72413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Transformation\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d7f21-c8d0-44da-b823-03be13b5568d",
   "metadata": {},
   "source": [
    "#### ***Code Explainer***\n",
    "- **`transforms.Compose`:** Creates a composition of multiple image transformations to be applied to the images.\n",
    "- **`transforms.Resize((224, 224))`:** Resizes each image to **224x224 pixels**.\n",
    "- **`transforms.RandomHorizontalFlip()`:** Applies horizontal flip to the images randomly. This helps in augmenting the dataset to improve generalization (not applied in the second set of transformations).\n",
    "- **`transforms.RandomRotation(10)`:** Rotates the images by a degree randomly selected from **(-10, 10)**, which again helps in dataset augmentation (not applied in the second set of transformations).\n",
    "- **`transforms.ToTensor()`:** Converts the images to PyTorch tensors (from *PIL images* or *NumPy arrays*).\n",
    "- **`transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])`:** Normalizes the tensor image with mean and standard deviation provided. These values are standard normalization means and stds for pre-trained models on *ImageNet*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d857faf1-65aa-4813-92c7-ae52e7e8cca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1pcTNgOsIHAgLzHBtMBoV0y-4fVMnvoNT\n",
      "From (redirected): https://drive.google.com/uc?id=1pcTNgOsIHAgLzHBtMBoV0y-4fVMnvoNT&confirm=t&uuid=8662600d-dd2e-434c-b9f4-a749d245a9f5\n",
      "To: C:\\Users\\jatin\\Desktop\\S3\\Internship\\RA-AIHI\\Assignments\\T2\\archive.zip\n",
      " 80%|███████▉  | 7.80G/9.77G [1:00:19<15:13, 2.15MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'archive.zip'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "file_id = '1pcTNgOsIHAgLzHBtMBoV0y-4fVMnvoNT'\n",
    "output = 'archive.zip'  # Replace with the desired path where you want to save the downloaded file\n",
    "gdown.download(f'https://drive.google.com/uc?id={file_id}', output, quiet=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1c42ea0-87ee-42ba-a139-3d022822a0df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m \n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mZipFile(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[0;32m      4\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/jatin/Desktop/S3/Internship/RA-AIHI/Assignments/T2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file' is not defined"
     ]
    }
   ],
   "source": [
    "import zipfile \n",
    "\n",
    "with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "    zip_ref.extractall('C:/Users/jatin/Desktop/S3/Internship/RA-AIHI/Assignments/T2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8543016-a3a9-4421-a8e1-3febb4b7dfcf",
   "metadata": {},
   "source": [
    "### **2. <u>Model Development<u/>:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f060ceb-b591-4993-9645-5f42f696e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model \n",
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 9)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0b815-6d24-4066-9618-6a8bc2db5572",
   "metadata": {},
   "source": [
    "#### ***Code Explainer***\n",
    "- **`models.resnet50(pretrained=True)`:** Loads a pre-trained *ResNet50 model* from PyTorch's model zoo, pre-trained on *ImageNet*.\n",
    "- **`num_ftrs = model.fc.in_features`:** Retrieves the number of input features for the final fully connected (*fc*) layer of the model.\n",
    "- **`model.fc = nn.Linear(num_ftrs, 9)`:** Replaces the existing fully connected layer with a new one that has **9 output features** (corresponding to 9 classes in your dataset).\n",
    "- **`model = model.to(device)`:** Moves the model to the specified device (GPU or CPU), allowing for computations to be performed on that device. This improves performance if a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72da7b-842e-4a0b-915b-a916cb1baceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ae965-76a6-4ac2-975c-365aee6fa1f3",
   "metadata": {},
   "source": [
    "#### ***Code Explainer***\n",
    "- **`criterion = nn.CrossEntropyLoss()`:** Defines the loss function as cross-entropy loss, which is commonly used for multi-class classification tasks. This loss function combines *Log Softmax* and *NLL Loss* in a single function.\n",
    "- **`optimizer = optim.Adam(model.parameters(), lr=0.001)`:** Sets up the Adam optimizer for adjusting model weights based on the calculated gradients, with a **learning rate** of **0.001**. Adam is an adaptive learning rate optimization algorithm that combines the benefits of AdaGrad and RMSProp to handle sparse gradients on noisy problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4c7f0-79b4-4c50-9987-752d554911c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader: \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0346cb-a7c3-4ccb-bdce-3444ee7694df",
   "metadata": {},
   "source": [
    "#### ***Code Explainer***\n",
    "- **`for epoch in range(num_epochs)`:** Initiates a loop over a specified number of epochs, allowing the model to learn from the data multiple times.\n",
    "- **`for inputs, labels in train_loader`:** Iterates over the training data loader, which supplies batches of inputs and corresponding labels.\n",
    "- **`inputs, labels = inputs.to(device), labels.to(device)`:** Moves the input data and labels to the configured device (either CPU or GPU), ensuring that the model operations are performed on the right device.\n",
    "- **`optimizer.zero_grad()`:** Clears old gradients from the last step before performing a new optimization step. This is necessary because gradients accumulate by default.\n",
    "- **`outputs = model(inputs)`:** Passes the input data through the model to get predictions.\n",
    "- **`loss = criterion(outputs, labels)`:** Computes the loss between the model's predictions and the actual labels using the cross-entropy loss function.\n",
    "- **`loss.backward()`:** Computes the derivative of the loss with respect to the parameters (gradient) using backpropagation.\n",
    "- **`optimizer.step()`:** Performs a single optimization step (parameter update).\n",
    "- **`running_loss += loss.item()`:** Aggregates the loss over each batch for monitoring.\n",
    "- **`print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')`:** Prints the average loss for each epoch, providing insight into how well the model is learning from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996bce62-9089-449a-8af0-b189004bd243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in val_loader:  # Corrected 'from' to 'for' and 'label' to 'labels'\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Corrected 'output' to 'outputs'\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'Validation accuracy: {100 * correct / total}%')  # Added calculation for percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be212a7e-aff0-4042-86a7-afcac4288b0e",
   "metadata": {},
   "source": [
    "#### ***Code Explainer***\n",
    "- **`model.eval()`:** Sets the model to evaluation mode. This is important as it tells the model to behave in an inference mode (e.g., turning off dropout).\n",
    "- **`with torch.no_grad()`:** Disables gradient calculation, which is beneficial for inference since it reduces memory usage and speeds up computation.\n",
    "- **`for images, labels in val_loader`:** Iterates over the validation data loader, which provides batches of images and labels for validation.\n",
    "- **`images, labels = images.to(device), labels.to(device)`:** Moves the images and labels to the appropriate device (CPU or GPU), ensuring computations are performed where expected.\n",
    "- **`outputs = model(images)`:** Computes the model's output predictions for the given batch of images.\n",
    "- **`_, predicted = torch.max(outputs.data, 1)`:** Finds the predicted class labels by selecting the class with the maximum output in the logits dimension.\n",
    "- **`total += labels.size(0)`:** Accumulates the total number of labels processed, which is used to calculate accuracy.\n",
    "- **`correct += (predicted == labels).sum().item()`:** Increments the correct predictions counter by the number of correct predictions in the batch.\n",
    "- **`print(f'Validation accuracy: {100 * correct / total}%')`:** Prints the validation accuracy as a percentage of correct predictions over the total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4d845-4097-4b1e-af43-3a68492758af",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8012446-f525-4cc9-b1b7-d9ff9f192518",
   "metadata": {},
   "source": [
    "### **3. <u>Evaluation & Tuning<u/>:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc71ae5-ef33-411e-9f42-d65fdc3051e1",
   "metadata": {},
   "source": [
    "### **4. <u>Deployment and Testing<u/>:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2dfc8-b9e4-4130-9750-c3a486d73b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
